{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "a514debe",
      "metadata": {
        "id": "a514debe"
      },
      "source": [
        "NEURAL NETWORKS:\n",
        "We will use the IRIS dataset to train our model.\n",
        "Every neural network has input layer, hidden layers(neurons, fully connected or partial), output layer\n",
        "Use Object Oriented programming for neural networks."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "1708412f",
      "metadata": {
        "id": "1708412f"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "c989230a",
      "metadata": {
        "id": "c989230a"
      },
      "source": [
        "Now create a model class that inherits nn.module, For this:\n",
        "Input-The 4 features of the flower ----> HL1 (x neurons) ----> HL2(y neurons) ----> Outpur - Gives the flower type\n",
        "In init: We give parameters as self, input satatypes, the hiddenlayers with given number of neurons and then the number of output features.\n",
        "We use nn.Linear(firstlayer,secondlayer) --> Data moves from left parameter to the right parameter\n",
        "We create another function called forward to move this data forward as well\n",
        "In forward:We Pass self and a x as the parameters\n",
        "Use F (functional)'s relu i.e rectified linear unit. This says do something ---> If o/p is <0 call it 0 if it's >0 now it goes to that number.\n",
        "The super__init__ command allows the initialisation of nn.Module"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "f70fc3c1",
      "metadata": {
        "id": "f70fc3c1"
      },
      "outputs": [],
      "source": [
        "class model(nn.Module):\n",
        "    def __init__(self,input_features=4,hl1=8,hl2=9,output_features=3):\n",
        "        super().__init__()\n",
        "        self.fc1=nn.Linear(input_features,hl1)\n",
        "        self.fc2=nn.Linear(hl1,hl2)\n",
        "        self.out=nn.Linear(hl2,output_features)\n",
        "    def forward(self,x):\n",
        "        x=F.relu(self.fc1(x))\n",
        "        x=F.relu(self.fc2(x))\n",
        "        x=self.out(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2022e071",
      "metadata": {
        "id": "2022e071"
      },
      "source": [
        "Now we must create a manual seed, this is because:\n",
        "1. These models involve randomisation i.e. different random result in each different run\n",
        "2. Seeding --> We tell the model to start the testing in the same way every time\n",
        "3. Don't seed the final product, never seed that.\n",
        "The  number passed as the seed parameter means nothing really, and model() creates an instance.\n",
        "The parameter doesn't matter but use the same parameter again and again each time otherwise the outputs will differ.\n",
        "42: The hitcchhiker's guide to galaxy\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "791ac344",
      "metadata": {
        "id": "791ac344"
      },
      "outputs": [],
      "source": [
        "torch.manual_seed(42)\n",
        "model=model()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d890a169",
      "metadata": {
        "id": "d890a169"
      },
      "source": [
        "This is the end of model creation NEXT WE TRAIN IT!!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5fac521f",
      "metadata": {
        "id": "5fac521f"
      },
      "source": [
        "To load and train our model with the IRIS dataset:\n",
        "Use pandas to load, matplotlib to graph it's accuracy %matplotlib inline will make sure it plots it on the notebook\n",
        "I changed the last column of strings to integers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "2711cfbf",
      "metadata": {
        "id": "2711cfbf"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "url = 'https://raw.githubusercontent.com/Dhruxp/MOSSAI/main/Data/iris.csv'\n",
        "my_df = pd.read_csv(url)\n",
        "my_df['class']=my_df['class'].map({'Iris-setosa':0,'Iris-versicolor':1,'Iris-virginica':2})"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "48c33c2c",
      "metadata": {
        "id": "48c33c2c"
      },
      "source": [
        "Now we train, test and split. Set x,y ---> x = input features, y = output features\n",
        "what we do is give x --> The training dataset i.e. the first for columns (DO this by dropping the last column)\n",
        "Note: Axis = 0 means row, Axis =1 means column\n",
        "Then convert to numpy arrays because these ML models will only and only care about numbers and raw calculation not about the labels, indexing etc."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "5a3c9697",
      "metadata": {
        "id": "5a3c9697"
      },
      "outputs": [],
      "source": [
        "X = my_df.drop('class',axis=1)\n",
        "y = my_df['class']\n",
        "x_np = X.values\n",
        "y_np = y.values"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "376b8b84",
      "metadata": {
        "id": "376b8b84"
      },
      "source": [
        "Then we import scikit-learn from which we can use train_test_split\n",
        "now to train the dataset, we give the np arrays + test sixe = some number ex 0.2 i.e. 20% of the data set is the test set everytime.\n",
        "random_state is like setting a model seed.\n",
        "Convert x --> float tensors because they have decimal points\n",
        "Convert y --> long tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "f5d13a94",
      "metadata": {
        "id": "f5d13a94"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train,X_test,y_train,y_test=train_test_split(x_np,y_np,test_size=0.2,random_state=42)\n",
        "X_train=torch.FloatTensor(X_train)\n",
        "X_test=torch.FloatTensor(X_test)\n",
        "y_train=torch.LongTensor(y_train)\n",
        "y_test=torch.LongTensor(y_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b8cf10f2",
      "metadata": {
        "id": "b8cf10f2"
      },
      "source": [
        "From here what we are gonna use is this simple thing call CrossEntropyLoss.\n",
        "The is an error criterion what it checks is correctness and confidence.\n",
        "Then it punishes or rewards the model accordingly. So the softmax is calculated for each using the softmaxformula, then it calculates loss using the negative log likelihood formula is the loss is huge it's heavily punished. In ideal case where loss =0, probability is 1 the model is rewarded.\n",
        "The neural network gives out raw logits (Confident scores) no probabilities."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "ceea7cf1",
      "metadata": {
        "id": "ceea7cf1"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "858da6f8",
      "metadata": {
        "id": "858da6f8"
      },
      "source": [
        "Now we use the adam optimiser basically it's a smarter way of using gradient descent\n",
        "Then we define learning rate i.e. if our model isn't correct after multiple tries slow down it's learning rate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "f238d46f",
      "metadata": {
        "id": "f238d46f"
      },
      "outputs": [],
      "source": [
        "optimizer = torch.optim.Adam(model.parameters(),lr=0.01)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b6f3099a",
      "metadata": {
        "id": "b6f3099a"
      },
      "source": [
        "Determine number of epochs: Number of iterations through the entire neural network\n",
        "Then we just take our losses into an array"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "69cfe261",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "69cfe261",
        "outputId": "5c70444b-cdd7-45ce-eba8-5f3fe8fdb235"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 and loss is 1.0846847295761108\n",
            "Epoch 10 and loss is 0.9183797240257263\n",
            "Epoch 20 and loss is 0.7622429728507996\n",
            "Epoch 30 and loss is 0.6273833513259888\n",
            "Epoch 40 and loss is 0.4667472541332245\n",
            "Epoch 50 and loss is 0.28770601749420166\n",
            "Epoch 60 and loss is 0.16738919913768768\n",
            "Epoch 70 and loss is 0.10598888993263245\n",
            "Epoch 80 and loss is 0.0802135244011879\n",
            "Epoch 90 and loss is 0.06947019696235657\n"
          ]
        }
      ],
      "source": [
        "epochs = 100\n",
        "losses = []\n",
        "for i in range(epochs):\n",
        "    y_pred = model.forward(X_train) # move forward\n",
        "    loss = criterion(y_pred,y_train) # calculate loss\n",
        "    losses.append(loss.detach().numpy()) # store loss for plotting\n",
        "    if i%10==0:\n",
        "        print(f'Epoch {i} and loss is {loss.item()}')\n",
        "    optimizer.zero_grad() # back propagation to fine tune the weights after error calculation\n",
        "    loss.backward()\n",
        "    optimizer.step()"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}