{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "03e56cb2",
      "metadata": {
        "id": "03e56cb2"
      },
      "source": [
        "We will now construct an actual cnn and not only layer by layer traversing, all of these are from the previous notebook coded as is these are basic requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "5a1c3209",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5a1c3209",
        "outputId": "72fcf905-93d5-46dd-d3c8-f3f14a2f929b"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 9.91M/9.91M [00:00<00:00, 16.2MB/s]\n",
            "100%|██████████| 28.9k/28.9k [00:00<00:00, 500kB/s]\n",
            "100%|██████████| 1.65M/1.65M [00:00<00:00, 4.44MB/s]\n",
            "100%|██████████| 4.54k/4.54k [00:00<00:00, 15.3MB/s]\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torchvision.utils import make_grid\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "data_dir = \"Data/mnist\"\n",
        "transform = transforms.Compose([transforms.ToTensor(),\n",
        "                                 transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root=data_dir, train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root=data_dir, train=False, transform=transform, download=True)\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fb5767a8",
      "metadata": {
        "id": "fb5767a8"
      },
      "source": [
        "Now we can construct the skeleton."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "60f637b8",
      "metadata": {
        "id": "60f637b8"
      },
      "outputs": [],
      "source": [
        "class cnn(nn.Module):\n",
        "    def __init__(self):\n",
        "        super().__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
        "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
        "        self.fc1 = nn.Linear(5*5*16, 120)\n",
        "        self.fc2 = nn.Linear(120, 84)\n",
        "        self.fc3 = nn.Linear(84, 10)\n",
        "    def forward(self, X):\n",
        "        X = F.relu(self.conv1(X))\n",
        "        X = F.max_pool2d(X,2,2)\n",
        "        X = F.relu(self.conv2(X)) #second pass\n",
        "        X = F.max_pool2d(X,2,2)\n",
        "        X = X.view(-1, 16*5*5) #flattening\n",
        "        X = F.relu(self.fc1(X))\n",
        "        X = F.relu(self.fc2(X))\n",
        "        X = F.relu(self.fc3(X))\n",
        "        return F.log_softmax(X, dim=1)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6d58b468",
      "metadata": {
        "id": "6d58b468"
      },
      "source": [
        "Now we will create an instance of our created model starting by setting a manual seed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "36727b00",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "36727b00",
        "outputId": "46315433-23ce-4913-d636-ad21d5aee2e3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cnn(\n",
            "  (conv1): Conv2d(1, 6, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (conv2): Conv2d(6, 16, kernel_size=(3, 3), stride=(1, 1))\n",
            "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
            "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
            "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
            ")\n"
          ]
        }
      ],
      "source": [
        "torch.manual_seed(41)\n",
        "model = cnn()\n",
        "print(model)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "50dfa199",
      "metadata": {
        "id": "50dfa199"
      },
      "source": [
        "Now loss function and optimizer CrossEnropyLoss and Adam optimizer as used before. Here smaller the learning rate, longer it'll take to train"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "772d1e0a",
      "metadata": {
        "id": "772d1e0a"
      },
      "outputs": [],
      "source": [
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8ba4ef45",
      "metadata": {
        "id": "8ba4ef45"
      },
      "source": [
        "Moving onto training and testing now. Time we'll use to see how long our CNN will take to run. Now we are going to keep track of our losses and correctness as we move forward in this program"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "696ff0a6",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "696ff0a6",
        "outputId": "fe8fa1da-2bd9-4edd-83e9-e351ab717c9b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch: 0 batch: 600 loss: 0.6762621998786926\n",
            "epoch: 1 batch: 600 loss: 0.3470820486545563\n",
            "epoch: 2 batch: 600 loss: 0.8898348212242126\n",
            "epoch: 3 batch: 600 loss: 0.765064537525177\n",
            "epoch: 4 batch: 600 loss: 0.9509320855140686\n",
            "Training took: 1.9006170670191447 mins\n"
          ]
        }
      ],
      "source": [
        "import time\n",
        "start_time = time.time()\n",
        "epochs = 5\n",
        "train_losses = []\n",
        "test_losses = []\n",
        "train_correct = []\n",
        "test_correct = []\n",
        "for i in range(epochs):\n",
        "    train_cor = 0\n",
        "    test_cor = 0\n",
        "    for b,(X_train, y_train) in enumerate(train_loader):\n",
        "        b+=1\n",
        "        y_pred = model(X_train)\n",
        "        loss = criterion(y_pred, y_train)\n",
        "        predicted = torch.max(y_pred.data, 1)[1]\n",
        "        batch_cor = (predicted == y_train).sum()\n",
        "        train_cor+=batch_cor\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        if (b%600 == 0):\n",
        "            print(f'epoch: {i} batch: {b} loss: {loss.item()}'\n",
        "            )\n",
        "    train_losses.append(loss)\n",
        "    train_correct.append(train_cor)\n",
        "    with torch.no_grad():\n",
        "        for b,(X_test, y_test) in enumerate(test_loader):\n",
        "            y_val = model(X_test)\n",
        "            predicted = torch.max(y_val.data, 1)[1]\n",
        "            test_cor+= (predicted == y_test).sum()\n",
        "    loss = criterion(y_val, y_test)\n",
        "    test_losses.append(loss)\n",
        "    test_correct.append(test_cor)\n",
        "current_time = time.time()\n",
        "total = current_time - start_time\n",
        "print(f'Training took: {total/60} mins')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "07ce3d8a",
      "metadata": {},
      "source": [
        "Now to graph these results basically graph the losses, before that convert the nmupy arrays to python lists"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "97a4a9bc",
      "metadata": {},
      "outputs": [],
      "source": [
        "train_losses = [tl.item() for tl in train_losses]\n",
        "test_losses = [tl.item() for tl in test_losses]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "83b91607",
      "metadata": {},
      "outputs": [],
      "source": [
        "plt.plot(train_losses, label='Training loss')\n",
        "plt.plot(test_losses, label='Testing loss')\n",
        "plt.title('Loss at epoch')\n",
        "plt.legend()"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
