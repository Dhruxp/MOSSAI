{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "03e56cb2",
   "metadata": {},
   "source": [
    "We will now construct an actual cnn and not only layer by layer traversing, all of these are from the previous notebook coded as is these are basic requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1c3209",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.utils import make_grid\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "data_dir = \"Data/mnist\"\n",
    "transform = transforms.Compose([transforms.ToTensor(),\n",
    "                                 transforms.Normalize((0.5,), (0.5,))])\n",
    "train_dataset = datasets.MNIST(root=data_dir, train=True, transform=transform, download=True)\n",
    "test_dataset = datasets.MNIST(root=data_dir, train=False, transform=transform, download=True)\n",
    "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb5767a8",
   "metadata": {},
   "source": [
    "Now we can construct the skeleton."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60f637b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cnn(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 3, 1)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 3, 1)\n",
    "        self.fc1 = nn.Linear(5*5*16, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "    def forward(self, X):\n",
    "        X = F.relu(self.conv1(X))\n",
    "        X = F.max_pool2d(X,2,2)\n",
    "        X = F.relu(self.conv2(X)) #second pass\n",
    "        X = F.max_pool2d(X,2,2)\n",
    "        X = X.view(-1, 16*5*5) #flattening\n",
    "        X = F.relu(self.fc1(X))\n",
    "        X = F.relu(self.fc2(X))\n",
    "        X = F.relu(self.fc3(X))\n",
    "        return F.log_softmax(X, dim=1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d58b468",
   "metadata": {},
   "source": [
    "Now we will create an instance of our created model starting by setting a manual seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36727b00",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(42)\n",
    "model = cnn()\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50dfa199",
   "metadata": {},
   "source": [
    "Now loss function and optimizer CrossEnropyLoss and Adam optimizer as used before. Here smaller the learning rate, longer it'll take to train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "772d1e0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr = 0.001) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ba4ef45",
   "metadata": {},
   "source": [
    "Moving onto training and testing now. Time we'll use to see how long our CNN will take to run. Now we are going to keep track of our losses and correctness as we move forward in this program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "696ff0a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_time = time.time()\n",
    "epochs = 5\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "train_correct = []\n",
    "test_correct = []\n",
    "for i in range(epochs):\n",
    "    train_cor = 0\n",
    "    test_cor = 0\n",
    "    for b,(X_train, y_train) in enumerate(train_loader):\n",
    "        b+=1\n",
    "        y_pred = model(X_train) \n",
    "        loss = criterion(y_pred, y_train)\n",
    "        predicted = torch.max(y_pred.data, 1)[1]\n",
    "        batch_cor = (predicted == y_train).sum()\n",
    "        train_cor+=batch_cor\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (b%600 == 0):\n",
    "            print(f'epoch: {i} batch: {b} loss: {loss.item()}'\n",
    "            )\n",
    "    train_losses.append(loss)\n",
    "    train_correct.append(train_cor)\n",
    "    with torch.no_grad():\n",
    "        for b,(X_test, y_test) in enumerate(test_loader):\n",
    "            y_val = model(X_test)\n",
    "            predicted = torch.max(y_val.data, 1)[1]\n",
    "            test_cor+= (predicted == y_test).sum()\n",
    "    loss = criterion(y_val, y_test)\n",
    "    test_losses.append(loss)\n",
    "    test_correct.append(test_cor)\n",
    "current_time = time.time()\n",
    "total = current_time - start_time\n",
    "print(f'Training took: {total/60} mins')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
