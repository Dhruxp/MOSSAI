{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "236ad088",
      "metadata": {
        "id": "236ad088"
      },
      "source": [
        "Now we will look how to create and perform operations on dataframes and understand what it does or looks like."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "dedd1851",
      "metadata": {
        "id": "dedd1851"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from numpy.random import randn\n",
        "my_data = randn(4,3) # 4 rows, 3 columns\n",
        "my_rows = ['row1', 'row2', 'row3', 'row4']\n",
        "my_cols = ['monday', 'tuesday', 'wednesday']"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7a6a15a0",
      "metadata": {
        "id": "7a6a15a0"
      },
      "source": [
        "Now we will actually create the dataframe and parameters go like data,rows,columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "94e9095a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "94e9095a",
        "outputId": "1d383fb6-f684-4fa1-d105-453d4891fd24"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "        monday   tuesday  wednesday\n",
            "row1 -0.203385  1.622957  -0.619391\n",
            "row2 -0.297136 -1.418792  -1.337988\n",
            "row3  0.521635  0.080755  -0.198449\n",
            "row4 -0.297743  0.775618  -0.118279\n"
          ]
        }
      ],
      "source": [
        "my_df = pd.DataFrame(data=my_data, index=my_rows, columns=my_cols)\n",
        "print(my_df)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "99c263b2",
      "metadata": {},
      "source": [
        "Now we will import a given dataset that is already a csv file and convert it directly to a dataframe.\n",
        "pd.read_csv will do this whole operation for us and save us the data pulling and pushing step. I will use the already existing iris data that I used in the neural network notebook in the same repo."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "31c32686",
      "metadata": {},
      "outputs": [],
      "source": [
        "my_df2 = pd.read_csv('iris.csv')\n",
        "print(my_df2)"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
